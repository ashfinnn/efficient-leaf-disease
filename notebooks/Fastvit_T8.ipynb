{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6ebf6747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install torch torchvision timm scikit-learn pillow matplotlib seaborn tqdm -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "06d20eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "↪ Resuming from epoch 19\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 20/20: 100%|██████████| 1601/1601 [12:39<00:00,  2.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "val_acc=0.9966  best=0.9966  patience=2\n",
      "✓ Metrics saved → evaluation_results\\fastvit_t8_metrics.json\n",
      "✓ Confusion matrix plot saved.\n",
      "Finished! Checkpoints in trained_models\\fastvit_t8\n"
     ]
    }
   ],
   "source": [
    "# run_fastvit_t8_highres_v3.py\n",
    "# Final high-resolution experiment for 24 IMC paper (with robust resume for scheduler)\n",
    "# Author: <your-name>          Date: 2025-08-xx\n",
    "\n",
    "import warnings, random, json, time\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch, torch.nn as nn, torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchvision import datasets, transforms\n",
    "import timm\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (accuracy_score, precision_score, recall_score,\n",
    "                             f1_score, confusion_matrix, classification_report)\n",
    "import matplotlib.pyplot as plt, seaborn as sns\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 1. CONFIGURATION\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "MODEL_NAME     = \"fastvit_t8\"\n",
    "ORIG_DATA_DIR  = Path(\"../datasets\")\n",
    "CKPT_DIR       = Path(\"trained_models\")/MODEL_NAME\n",
    "LOG_CSV        = Path(\"logs\")/f\"{MODEL_NAME}.csv\"\n",
    "OUT_JSON       = Path(\"evaluation_results\")/f\"{MODEL_NAME}_metrics.json\"\n",
    "\n",
    "IMG_SIZE       = 224\n",
    "BATCH          = 8\n",
    "ACC_STEPS      = 4\n",
    "FROZEN_EPOCHS  = 5\n",
    "TOTAL_EPOCHS   = 20\n",
    "PATIENCE       = 5\n",
    "LR             = 1e-4\n",
    "WD             = 1e-4\n",
    "WORKERS        = 4\n",
    "SEED           = 42\n",
    "USE_COMPILE    = False\n",
    "DEVICE         = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "AMP            = torch.cuda.is_available()\n",
    "\n",
    "for p in [\"trained_models\", \"logs\", \"evaluation_results\"]:\n",
    "    Path(p).mkdir(exist_ok=True)\n",
    "\n",
    "torch.manual_seed(SEED); np.random.seed(SEED); random.seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 2. DATA TRANSFORMS & LOADERS\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def build_transforms():\n",
    "    norm = transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    train_tf = transforms.Compose([transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8,1.0)), transforms.RandomHorizontalFlip(), transforms.ToTensor(), norm])\n",
    "    val_tf   = transforms.Compose([transforms.Resize(int(IMG_SIZE*1.12)), transforms.CenterCrop(IMG_SIZE), transforms.ToTensor(), norm])\n",
    "    return train_tf, val_tf\n",
    "\n",
    "full_ds = datasets.ImageFolder(ORIG_DATA_DIR)\n",
    "CLASS_NAMES = full_ds.classes\n",
    "NUM_CLASSES = len(CLASS_NAMES)\n",
    "train_idx, val_idx = train_test_split(np.arange(len(full_ds.targets)), test_size=0.2, stratify=full_ds.targets, random_state=SEED)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 3. HELPERS\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def freeze_backbone(model, train_full=False):\n",
    "    for n,p in model.named_parameters():\n",
    "        p.requires_grad = train_full or any(k in n for k in (\"head\",\"fc\",\"classifier\"))\n",
    "\n",
    "def maybe_compile(m):\n",
    "    if USE_COMPILE and hasattr(torch,\"compile\"):\n",
    "        try: return torch.compile(m, dynamic=False)\n",
    "        except Exception as e: print(\"torch.compile disabled →\", e)\n",
    "    return m\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval(); preds=[]; labels=[]\n",
    "    with torch.no_grad(), torch.cuda.amp.autocast(enabled=AMP):\n",
    "        for xb,yb in loader:\n",
    "            preds.extend(model(xb.to(DEVICE)).argmax(1).cpu().numpy())\n",
    "            labels.extend(yb.numpy())\n",
    "    return np.array(preds), np.array(labels)\n",
    "\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "# 4. TRAINING LOOP\n",
    "# ──────────────────────────────────────────────────────────────\n",
    "def main():\n",
    "    train_tf, val_tf = build_transforms()\n",
    "    train_ds = Subset(full_ds, train_idx); train_ds.dataset.transform = train_tf\n",
    "    val_ds   = Subset(full_ds, val_idx);   val_ds.dataset.transform   = val_tf\n",
    "    train_ld = DataLoader(train_ds, BATCH, True , num_workers=WORKERS, pin_memory=True)\n",
    "    val_ld   = DataLoader(val_ds,   BATCH, False, num_workers=WORKERS, pin_memory=True)\n",
    "\n",
    "    model = timm.create_model(MODEL_NAME, pretrained=True, num_classes=NUM_CLASSES).to(DEVICE)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    scaler = torch.cuda.amp.GradScaler(enabled=AMP)\n",
    "\n",
    "    start_ep, best_acc, patience_left = 0, 0., PATIENCE\n",
    "    optimizer_state, scaler_state, scheduler_state = None, None, None # --- MODIFIED ---\n",
    "\n",
    "    if (CKPT_DIR/\"last.pth\").is_file():\n",
    "        chk = torch.load(CKPT_DIR/\"last.pth\", map_location=DEVICE)\n",
    "        model.load_state_dict(chk[\"model\"])\n",
    "        start_ep       = chk[\"epoch\"] + 1\n",
    "        best_acc       = chk[\"best_acc\"]\n",
    "        patience_left  = chk[\"patience\"]\n",
    "        optimizer_state = chk[\"optim\"]\n",
    "        scaler_state    = chk[\"scaler\"]\n",
    "        scheduler_state = chk.get(\"scheduler\") # --- MODIFIED --- .get() is safer\n",
    "        print(f\"↪ Resuming from epoch {start_ep}\")\n",
    "\n",
    "    if start_ep < FROZEN_EPOCHS:\n",
    "        freeze_backbone(model, False)\n",
    "        optimizer = optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=LR, weight_decay=WD)\n",
    "    else:\n",
    "        freeze_backbone(model, True)\n",
    "        optimizer = optim.AdamW(model.parameters(), lr=LR*0.5, weight_decay=WD)\n",
    "\n",
    "    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TOTAL_EPOCHS) # --- MODIFIED --- No last_epoch needed\n",
    "\n",
    "    if optimizer_state: optimizer.load_state_dict(optimizer_state)\n",
    "    if scaler_state: scaler.load_state_dict(scaler_state)\n",
    "    if scheduler_state: scheduler.load_state_dict(scheduler_state) # --- MODIFIED ---\n",
    "\n",
    "    model = maybe_compile(model)\n",
    "\n",
    "    if start_ep==0 and LOG_CSV.exists(): LOG_CSV.unlink()\n",
    "\n",
    "    for ep in range(start_ep, TOTAL_EPOCHS):\n",
    "        if ep == FROZEN_EPOCHS:\n",
    "            freeze_backbone(model, True)\n",
    "            optimizer = optim.AdamW(model.parameters(), lr=LR*0.5, weight_decay=WD)\n",
    "            # Recreate scheduler for the new optimizer\n",
    "            scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=TOTAL_EPOCHS, last_epoch=ep-1)\n",
    "\n",
    "        model.train(); seen=0; loss_sum=0; correct=0\n",
    "        optimizer.zero_grad()\n",
    "        for i,(xb,yb) in enumerate(tqdm(train_ld, desc=f\"Epoch {ep+1}/{TOTAL_EPOCHS}\")):\n",
    "            xb,yb = xb.to(DEVICE), yb.to(DEVICE)\n",
    "            with torch.cuda.amp.autocast(enabled=AMP):\n",
    "                out = model(xb); loss = criterion(out,yb)/ACC_STEPS\n",
    "            scaler.scale(loss).backward()\n",
    "            if (i+1)%ACC_STEPS==0:\n",
    "                scaler.step(optimizer); scaler.update(); optimizer.zero_grad()\n",
    "            loss_sum += loss.item()*ACC_STEPS*xb.size(0)\n",
    "            correct  += (out.argmax(1)==yb).sum().item()\n",
    "            seen     += xb.size(0)\n",
    "        train_loss, train_acc = loss_sum/seen, correct/seen\n",
    "\n",
    "        preds, labels = evaluate(model, val_ld)\n",
    "        val_acc = accuracy_score(labels, preds)\n",
    "        scheduler.step()\n",
    "\n",
    "        with open(LOG_CSV,\"a\") as f:\n",
    "            if ep==0: f.write(\"epoch,train_loss,train_acc,val_acc,lr\\n\")\n",
    "            f.write(f\"{ep},{train_loss:.5f},{train_acc:.5f},{val_acc:.5f},{scheduler.get_last_lr()[0]:.6f}\\n\")\n",
    "        print(f\"val_acc={val_acc:.4f}  best={best_acc:.4f}  patience={patience_left}\")\n",
    "\n",
    "        # --- MODIFIED ---: Save scheduler state in checkpoint\n",
    "        state = {\"epoch\":ep,\"model\":model.state_dict(),\"optim\":optimizer.state_dict(),\n",
    "                 \"scaler\":scaler.state_dict(),\"scheduler\":scheduler.state_dict(),\n",
    "                 \"best_acc\":best_acc,\"patience\":patience_left}\n",
    "        CKPT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "        torch.save(state, CKPT_DIR/\"last.pth\")\n",
    "        torch.save(state, CKPT_DIR/f\"epoch{ep:03d}.pth\")\n",
    "        \n",
    "        if val_acc > best_acc:\n",
    "            best_acc = val_acc\n",
    "            patience_left = PATIENCE\n",
    "            torch.save(state, CKPT_DIR/\"best.pth\")\n",
    "        else:\n",
    "            patience_left -= 1\n",
    "        \n",
    "        if patience_left == 0 and ep >= FROZEN_EPOCHS:\n",
    "            print(\"Early-stopping triggered.\")\n",
    "            break\n",
    "\n",
    "    # Final Evaluation\n",
    "    best_state = torch.load(CKPT_DIR/\"best.pth\", map_location=DEVICE)\n",
    "    model.load_state_dict(best_state[\"model\"])\n",
    "    preds, labels = evaluate(model, val_ld)\n",
    "    t0=time.time()\n",
    "    with torch.no_grad():\n",
    "        for xb,_ in val_ld: model(xb.to(DEVICE))\n",
    "    latency = (time.time()-t0)/len(val_ds)\n",
    "    metrics = dict(model=MODEL_NAME, image_size=IMG_SIZE, val_accuracy=accuracy_score(labels, preds), precision=precision_score(labels, preds, average=\"weighted\", zero_division=0), recall=recall_score(labels, preds, average=\"weighted\",  zero_division=0), f1_score=f1_score(labels, preds,  average=\"weighted\", zero_division=0), inf_sec_per_img=latency, conf_matrix=confusion_matrix(labels, preds).tolist(), class_report=classification_report(labels, preds, target_names=CLASS_NAMES, zero_division=0, output_dict=True))\n",
    "    OUT_JSON.parent.mkdir(exist_ok=True)\n",
    "    json.dump(metrics, open(OUT_JSON,\"w\"), indent=2)\n",
    "    print(\"✓ Metrics saved →\", OUT_JSON)\n",
    "    sns.heatmap(metrics[\"conf_matrix\"], cmap=\"Blues\", cbar=False, annot=False, xticklabels=CLASS_NAMES, yticklabels=CLASS_NAMES)\n",
    "    plt.title(f\"{MODEL_NAME} Confusion Matrix\"); plt.tight_layout()\n",
    "    plt.savefig(OUT_JSON.with_suffix(\".png\"), dpi=300); plt.close()\n",
    "    print(\"✓ Confusion matrix plot saved.\")\n",
    "    print(\"Finished! Checkpoints in\", CKPT_DIR)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ce6877b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Obidur Rahman\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Architecture Verification\n",
      "==================================================\n",
      "\n",
      "RESNET50:\n",
      "  Parameters: 25.56M (25,557,032 total)\n",
      "  Input Size: (3, 224, 224)\n",
      "  Pretrained Available: True\n",
      "  Architecture Family: resnet50\n",
      "\n",
      "CONVNEXT_TINY:\n",
      "  Parameters: 28.59M (28,589,128 total)\n",
      "  Input Size: (3, 224, 224)\n",
      "  Pretrained Available: True\n",
      "  Architecture Family: convnext_tiny\n",
      "\n",
      "FASTVIT_T8:\n",
      "  Parameters: 4.03M (4,026,232 total)\n",
      "  Input Size: (3, 256, 256)\n",
      "  Pretrained Available: True\n",
      "  Architecture Family: fastvit_t8\n",
      "\n",
      "==================================================\n",
      "Available FastViT variants in TIMM:\n",
      "  - fastvit_ma36\n",
      "  - fastvit_mci0\n",
      "  - fastvit_mci1\n",
      "  - fastvit_mci2\n",
      "  - fastvit_s12\n",
      "  - fastvit_sa12\n",
      "  - fastvit_sa24\n",
      "  - fastvit_sa36\n",
      "  - fastvit_t8\n",
      "  - fastvit_t12\n",
      "\n",
      "==================================================\n",
      "Testing model creation with 10-class classifier:\n",
      "resnet50: 23.53M parameters (with 10-class classifier)\n",
      "convnext_tiny: 27.83M parameters (with 10-class classifier)\n"
     ]
    }
   ],
   "source": [
    "import timm\n",
    "import torch\n",
    "\n",
    "def count_parameters(model):\n",
    "    \"\"\"Count the number of parameters in a model\"\"\"\n",
    "    return sum(p.numel() for p in model.parameters())\n",
    "\n",
    "def get_model_info(model_name):\n",
    "    \"\"\"Get detailed information about a model\"\"\"\n",
    "    try:\n",
    "        # Check if pretrained weights are available\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=1000)\n",
    "        \n",
    "        # Count parameters\n",
    "        total_params = count_parameters(model)\n",
    "        trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        # Get model configuration\n",
    "        config = model.default_cfg\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'total_parameters': total_params,\n",
    "            'trainable_parameters': trainable_params,\n",
    "            'parameters_M': round(total_params / 1e6, 2),\n",
    "            'input_size': config.get('input_size', 'Unknown'),\n",
    "            'pretrained_available': True,\n",
    "            'architecture_family': config.get('architecture', 'Unknown')\n",
    "        }\n",
    "    except Exception as e:\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'error': str(e),\n",
    "            'pretrained_available': False\n",
    "        }\n",
    "\n",
    "# Models to verify\n",
    "models_to_check = [\n",
    "    'resnet50',\n",
    "    'convnext_tiny',\n",
    "    'fastvit_t8'\n",
    "]\n",
    "\n",
    "print(\"Model Architecture Verification\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "for model_name in models_to_check:\n",
    "    print(f\"\\n{model_name.upper()}:\")\n",
    "    info = get_model_info(model_name)\n",
    "    \n",
    "    if 'error' not in info:\n",
    "        print(f\"  Parameters: {info['parameters_M']}M ({info['total_parameters']:,} total)\")\n",
    "        print(f\"  Input Size: {info['input_size']}\")\n",
    "        print(f\"  Pretrained Available: {info['pretrained_available']}\")\n",
    "        print(f\"  Architecture Family: {info['architecture_family']}\")\n",
    "    else:\n",
    "        print(f\"  Error: {info['error']}\")\n",
    "\n",
    "# Check available FastViT variants\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Available FastViT variants in TIMM:\")\n",
    "try:\n",
    "    fastvit_models = timm.list_models('fastvit*')\n",
    "    for model in fastvit_models:\n",
    "        print(f\"  - {model}\")\n",
    "        \n",
    "    if not fastvit_models:\n",
    "        print(\"  No FastViT models found. Checking alternative names...\")\n",
    "        # Try alternative names\n",
    "        alt_names = ['fast_vit*', 'mobilevit*', 'efficientformer*']\n",
    "        for alt in alt_names:\n",
    "            models = timm.list_models(alt)\n",
    "            if models:\n",
    "                print(f\"  Alternative models ({alt}):\")\n",
    "                for model in models[:5]:  # Show first 5\n",
    "                    print(f\"    - {model}\")\n",
    "except Exception as e:\n",
    "    print(f\"  Error checking FastViT models: {e}\")\n",
    "\n",
    "# Verify model creation with custom classifier\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"Testing model creation with 10-class classifier:\")\n",
    "\n",
    "for model_name in ['resnet50', 'convnext_tiny']:  # Skip fastvit_t8 if problematic\n",
    "    try:\n",
    "        model = timm.create_model(model_name, pretrained=True, num_classes=10)\n",
    "        params = count_parameters(model)\n",
    "        print(f\"{model_name}: {params/1e6:.2f}M parameters (with 10-class classifier)\")\n",
    "    except Exception as e:\n",
    "        print(f\"{model_name}: Error - {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfeb8f01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
